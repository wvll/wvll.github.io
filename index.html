<!DOCTYPE html>
<html lang="en">
  <head>
    <base href="." />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="resource-type" content="document" />
    <meta name="distribution" content="global" />
    
    <meta name="description" content="Workshop on Vision-Based Understanding and Linguistics (WVLL) - Join us for discussions on vision-based understanding challenges in low-resource languages. Explore document image processing, OCR, NLP, and more." />
    <meta name="keywords" content="Document Image Processing, Physical And Logical Layout Analysis, Optical Character Recognition (OCR), Document Classification, Natural Language Processing (NLP) For Document Understanding, Scene Text Detection And Recognition, Recognition Of Tables And Formulas In Documents, Document Summarization And Translation, Medical Document Analysis, Gold-Standard Benchmarks And Datasets For Low-Resource Languages, Video And Speech Analysis For Low-Resource Languages, Generative AI For Low-Resource Languages" />
    <meta name="author" content="Fuad Rahman, Syed Akhter Hossain, Sheikh Abujar, AKM Shahariar Azad Rabby, Muntaser Syed, Mouhaydine Tlemcani, Tozammel Hossain, Tazin Afrin, Ting Xiao, Sadia Afroz" />
    <meta name="robots" content="index, follow" />
    <meta name="geo.region" content="US-HI" />
    <meta name="geo.placename" content="WAIKOLOA" />
    <meta name="geo.position" content="19.937248;-155.791068" />
    <meta name="ICBM" content="19.937248, -155.791068" />
    <meta property="og:title" content="Workshop on Vision-Based Understanding and Linguistics (WVLL)" />
    <meta property="og:description" content="Join us for discussions on vision-based understanding challenges in low-resource languages. Explore document image processing, OCR, NLP, and more." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://wvll.github.io" />
    <meta property="og:image" content="https://wvll.github.io/assets/conference-image.jpg" />
    <meta property="og:locale" content="en_US" />
    <meta property="article:author" content="https://www.linkedin.com/in/fuadrahman, https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html, https://sites.google.com/site/iamabujarsheikh, https://rabby.dev, https://www.linkedin.com/in/muntasersyed, https://www.uevora.pt/pessoas?id=5279, https://facultyinfo.unt.edu/faculty-profile?profile=kh0718, https://tazin-afrin.github.io, https://engineering.unt.edu/people/ting-xiao.html, https://www.icsi.berkeley.edu/icsi/people/sadia" />
    <meta property="article:publisher" content="https://www.linkedin.com/in/fuadrahman, https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html, https://sites.google.com/site/iamabujarsheikh, https://rabby.dev, https://www.linkedin.com/in/muntasersyed, https://www.uevora.pt/pessoas?id=5279, https://facultyinfo.unt.edu/faculty-profile?profile=kh0718, https://tazin-afrin.github.io, https://engineering.unt.edu/people/ting-xiao.html, https://www.icsi.berkeley.edu/icsi/people/sadia" />
    <meta property="article:published_time" content="2024-01-03" />
    <meta property="article:modified_time" content="2024-01-07" />
    <meta property="og:site_name" content="WVLL Conference" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Workshop on Vision-Based Understanding for Low-Resource Languages (WVLL)" />
    <meta name="twitter:description" content="Join us for discussions on vision-based understanding challenges in low-resource languages. Explore document image processing, OCR, NLP, and more." />
    <meta name="twitter:image" content="https://wvll.github.io/assets/conference-image.jpg" />
    
    <title>WVLL - Workshop on Vision-Based Understanding and Linguistics</title>
    <style>
      :root {
        --bg-color: #f4f7f9; /* Light grayish blue */
        --primary-text-color: #333740; /* Dark gray for text */
        --secondary-text-color: #5a6270;
        --heading-color: #2c3e50; /* Darker blue/charcoal for headings */
        --accent-color: #2a9d8f; /* Teal accent */
        --accent-hover-color: #21867a; /* Darker teal for hover */
        --border-color: #dde4e9;
        --card-bg-color: #ffffff;
        --nav-bg-color: #2c3e50; /* Dark background for nav */
        --nav-link-color: #ecf0f1;
        --nav-link-hover-bg: var(--accent-color);

        /* Variables for hamburger menu (adapted from original) */
        --ham-label-bg: transparent; /* Make hamburger label bg transparent */
        --ham-border: var(--accent-color);
        --ham-line-color: var(--accent-color);
        --ham-menu-bg: var(--card-bg-color);
        --ham-link-color: var(--primary-text-color);
        --ham-link-current-color: var(--accent-color);
        --ham-menu-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
      }

      /* Global Resets & Base Styles */
      *,
      *::before,
      *::after {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      html {
        scroll-behavior: smooth;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
        /* Consider: font-family: 'Inter', sans-serif; if using Google Fonts */
        line-height: 1.7;
        color: var(--primary-text-color);
        background-color: var(--bg-color);
        font-size: 16px;
      }

      /* Typography */
      h1, h2, h3, h4, h5, h6 {
        color: var(--heading-color);
        margin-top: 1.8em;
        margin-bottom: 0.8em;
        line-height: 1.3;
      }

      h2 {
        font-size: 2.2rem; /* 35.2px */
        padding-bottom: 0.4em;
        border-bottom: 3px solid var(--accent-color);
      }

      h3 {
        font-size: 1.6rem; /* 25.6px */
        color: var(--accent-color);
        margin-top: 2em;
      }
      
      p {
        margin-bottom: 1.2em;
        color: var(--secondary-text-color);
      }

      a {
        color: var(--accent-color);
        text-decoration: none;
        transition: color 0.3s ease;
      }

      a:hover {
        color: var(--accent-hover-color);
        text-decoration: underline;
      }

      ul, ol {
        margin-bottom: 1em;
        padding-left: 20px; /* Default padding for lists */
      }
      
      li {
        margin-bottom: 0.5em;
      }

      /* Layout */
      .container {
        max-width: 1100px;
        margin: 30px auto;
        padding: 30px;
        background-color: var(--card-bg-color);
        box-shadow: 0 6px 18px rgba(44, 62, 80, 0.08);
        border-radius: 10px;
      }

      /* Banner */
      .banner {
        background: linear-gradient(135deg, var(--heading-color) 0%, var(--accent-color) 100%);
        color: white;
        padding: 50px 20px;
        text-align: center;
        border-bottom: 5px solid var(--accent-hover-color);
      }

      .banner-title {
        font-size: 2.8rem; /* 44.8px */
        font-weight: 700;
        margin-bottom: 0.3em;
        text-shadow: 1px 1px 3px rgba(0,0,0,0.3);
      }

      .banner-date {
        font-size: 1.3rem; /* 20.8px */
        opacity: 0.9;
      }
      
      .logos { /* Example styling for logos if they exist */
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 25px;
        margin-top: 25px;
      }
      .logos img { /* Assuming you'll add img tags here */
        max-height: 45px;
        filter: brightness(0) invert(1) opacity(0.8); /* Make them white and slightly transparent */
        transition: opacity 0.3s ease;
      }
      .logos img:hover {
        opacity: 1;
      }
      .spn { /* For "Sponsors" text or small logos */
          margin-top: 20px;
          font-size: 0.9rem;
          opacity: 0.7;
      }
      .spn_txt{
        /* Style as needed */
      }
      .spn_img {
         /* Style as needed */
      }


      /* Desktop Navigation */
      .desktop-navigation {
        background-color: var(--nav-bg-color);
        padding: 15px 0;
        position: sticky;
        top: 0;
        z-index: 1000;
        box-shadow: 0 2px 5px rgba(0,0,0,0.2);
      }
      .desktop-navigation ul {
        list-style: none;
        display: flex;
        justify-content: center;
        align-items: center;
      }
      .desktop-navigation ul li {
        margin: 0 10px;
      }
      .desktop-navigation ul li a {
        color: var(--nav-link-color);
        font-weight: 500; /* Medium weight for nav links */
        padding: 10px 18px;
        border-radius: 5px;
        transition: background-color 0.3s ease, color 0.3s ease;
        font-size: 1.05rem; /* 16.8px */
      }
      .desktop-navigation ul li a:hover,
      .desktop-navigation ul li a.current {
        background-color: var(--nav-link-hover-bg);
        color: white;
        text-decoration: none;
      }

      /* Key Areas Styling */
      .key-areas-list {
        column-count: 2;
        column-gap: 40px;
        padding-left: 0; /* Remove default ul padding if using custom list style */
      }
      .key-areas-list ul {
        list-style-type: none; /* Remove default bullets */
        padding-left: 0;
      }
      .key-areas-list li {
        position: relative;
        padding-left: 25px; /* Space for custom bullet */
        margin-bottom: 10px; /* Space between items */
      }
      .key-areas-list li::before {
        content: 'âœ”'; /* Custom bullet */
        position: absolute;
        left: 0;
        color: var(--accent-color);
        font-weight: bold;
      }


      /* Committee Table */
      .committee-table {
          width: 100%;
          border-collapse: collapse;
          margin-top: 25px;
          box-shadow: 0 3px 10px rgba(0,0,0,0.07);
          border-radius: 8px;
          overflow: hidden; /* Ensures border-radius clips content */
      }
      .committee-table th, .committee-table td {
          border: 1px solid var(--border-color);
          padding: 14px 18px;
          text-align: left;
      }
      .committee-table th {
          background-color: var(--heading-color);
          color: white;
          font-weight: 600; /* Bolder */
          font-size: 1.05rem;
      }
      .committee-table td {
        color: var(--secondary-text-color);
      }
      .committee-table tbody tr:nth-child(even) {
          background-color: #fbfcfd; /* Very light alternate row color */
      }
      .committee-table tbody tr:hover {
          background-color: #f0f4f7; /* Hover effect for rows */
      }


      /* Hamburger Menu (Adapted from original) */
      #ham-menu {
        display: none;
      }
      label[for="ham-menu"] { /* Hamburger Icon Label */
        display: none; /* Hidden by default, shown on mobile */
        position: fixed; /* Fixed position for mobile */
        top: 20px;
        left: 20px;
        z-index: 1002; /* Above content, below menu overlay */
        width: 50px;
        height: 50px;
        background-color: var(--ham-label-bg);
        border-radius: 8px;
        border: 2px solid var(--ham-border);
        cursor: pointer;
        transition: background-color 0.3s;
      }
      label[for="ham-menu"]:hover {
        background-color: rgba(42, 157, 143, 0.1); /* Light accent on hover */
      }

      .ham-menu { /* The menu itself */
        width: 80vw; /* Smaller width */
        max-width: 300px; /* Max width */
        height: 100%;
        position: fixed;
        top: 0;
        left: 0; /* Start off-screen */
        visibility: hidden;
        transform: translateX(-100%);
        z-index: 1001; /* Below icon, above page content but below overlay */
        background-color: var(--ham-menu-bg);
        transition: transform 0.5s ease, visibility 0.5s;
        display: flex;
        flex-direction: column; /* Align items vertically */
        padding-top: 80px; /* Space for close button or header */
        box-shadow: var(--ham-menu-shadow);
      }
      .ham-menu > ul {
        display: flex;
        flex-flow: column nowrap;
        width: 100%;
        padding: 20px;
      }
      .ham-menu a {
        display: block;
        padding: 15px 20px;
        font-size: 1.3rem; /* 20.8px */
        color: var(--ham-link-color);
        text-decoration: none;
        transition: background-color 0.3s, color 0.3s;
        border-radius: 5px;
        margin-bottom: 5px;
      }
      .ham-menu a:hover {
        background-color: var(--accent-color);
        color: white;
      }
      .ham-menu a.current {
        color: var(--ham-link-current-color);
        font-weight: bold;
        background-color: rgba(42, 157, 143, 0.1);
      }
       .ham-menu > ul > li {
        list-style-type: none;
      }

      #ham-menu:checked ~ .ham-menu {
        transform: translateX(0);
        visibility: visible;
      }
      
      .full-page-green { /* Overlay */
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-color: rgba(0, 0, 0, 0.5); /* Semi-transparent black */
        z-index: 1000; /* Below menu icon and menu, above content */
        opacity: 0;
        visibility: hidden;
        transition: opacity 0.5s, visibility 0.5s;
      }
      #ham-menu:checked ~ .full-page-green {
        opacity: 1;
        visibility: visible;
      }

      /* Hamburger Icon Lines */
      [for="ham-menu"] > div {
        width: 100%;
        height: 100%;
        display: flex;
        flex-flow: column wrap; /* Changed from column wrap to column */
        justify-content: center; /* Vertically center lines */
        align-items: center; /* Horizontally center lines */
      }
      .menu-line {
        display: block;
        width: 22px; /* Slightly wider lines */
        height: 3px; /* Thicker lines */
        margin: 3px 0; /* Adjust spacing */
        background-color: var(--ham-line-color);
        border-radius: 3px;
        transition: transform 0.4s ease, opacity 0.3s ease;
      }
      /* Animation for X */
      #ham-menu:checked + label .menu-line:nth-child(1) {
        transform: translateY(9px) rotate(45deg);
      }
      #ham-menu:checked + label .menu-line:nth-child(2) {
        opacity: 0;
        transform: scale(0);
      }
      #ham-menu:checked + label .menu-line:nth-child(3) {
        transform: translateY(-9px) rotate(-45deg);
      }
      /* Original complex line structure was simplified above to 3 lines */
      /* Removing styles for .menu-line:nth-child(4,5,6) as they are not needed for a standard 3-line hamburger */


      /* Responsive Adjustments */
      @media only screen and (max-width: 768px) {
        body {
          font-size: 15px;
        }
        .container {
          padding: 20px;
          margin: 20px 15px; /* Add side margin on mobile */
          border-radius: 8px;
        }
        .desktop-navigation {
          display: none; /* Hide desktop nav */
        }
        label[for="ham-menu"] { /* Hamburger Icon */
          display: flex; /* Show on mobile, flex for centering lines */
        }
        .banner-title {
          font-size: 2rem; /* Adjust banner title */
        }
        .banner-date {
          font-size: 1rem;
        }
        h2 {
          font-size: 1.8rem;
        }
        h3 {
          font-size: 1.4rem;
        }
        .key-areas-list {
          column-count: 1; /* Single column on mobile */
        }
         /* Adjust hamburger icon position if banner is present */
        label[for="ham-menu"].with-banner-margin {
          top: 20px; /* Adjust as needed if sticky nav or banner takes space */
        }
      }
      
      @media only screen and (min-width: 769px) {
        .hamble { /* Container for hamburger elements */
            display: none; /* Hide all hamburger-related elements on desktop */
        }
      }

    </style>
  </head>

  <body>
    <header class="banner">
      <div class="banner-content">
        <h1 class="banner-title">Workshop on Vision-Based Understanding and Linguistics (WVLL)</h1>
        <p class="banner-date">January 8, 2024 &bull; Waikoloa, Hawaii</p>
      </div>
      <div class="logos">
        </div>
       <div class="spn">
         </div>
    </header>

    <nav class="desktop-navigation">
      <ul>
        <li><a class="current" title="Conference Home Page" href="#overview">Home</a></li>
        <li><a title="About the Conference" href="#overview">About</a></li>
        <li><a title="Invited Speakers" href="#speakers">Speakers</a></li>
        <li><a title="Organizers of the Conference" href="#organizers">Organizers</a></li>
        <li><a title="Program Committee" href="#program-committee">Program Committee</a></li>
        <li><a title="Conference Social Impact" href="#diversity-plan">Social Impact</a></li>
        <li><a title="Previous Workshop" href="#previous-workshop">Previous Edition</a></li>
      </ul>
    </nav>

    <div class="hamble"> <input type="checkbox" id="ham-menu" />
      <label for="ham-menu"> <div class="hide-des"> <span class="menu-line"></span>
          <span class="menu-line"></span>
          <span class="menu-line"></span>
          </div>
      </label>
      <div class="full-page-green"></div> <nav class="ham-menu"> <ul>
            <li><a class="current" title="Conference Home Page" href="#overview">Home</a></li>
            <li><a title="About the Conference" href="#overview">About</a></li>
            <li><a title="Invited Speakers" href="#speakers">Speakers</a></li>
            <li><a title="Organizers of the Conference" href="#organizers">Organizers</a></li>
            <li><a title="Program Committee" href="#program-committee">Program Committee</a></li>
            <li><a title="Conference Social Impact" href="#diversity-plan">Social Impact</a></li>
            <li><a title="Previous Workshop" href="#previous-workshop">Previous Edition</a></li>
            </ul>
      </nav>
    </div>

    <main class="container">
      <section id="overview">
        <h2>Workshop Overview</h2>
        <p>
          The "2nd Workshop on Vision-Based Understanding and Linguistics (WVLL)" aims to create a dynamic and interactive forum for researchers exploring the rapidly evolving intersection of computer vision, natural language processing, and linguistic principles to achieve deeper and more nuanced machine understanding. As vision-language models (VLMs) demonstrate increasingly sophisticated capabilities, WVLL will focus on the critical challenges and opportunities that lie ahead, emphasizing the development of models that are not only powerful but also efficient, equitable, and grounded in a robust understanding of both visual and linguistic structures.
        </p>

        <h2>Key Areas of Exploration:</h2>
        <div class="key-areas-list">
          <ul>
            <li>AI For Low-Resource Languages</li>
            <li>Video And Speech Analysis For Low-Resource Languages</li>
            <li>LLM and VLM Architectures and Neural Design</li>
            <li>Parameter-Efficient Adaptation of Large Vision-Language Models</li>
            <li>Applications in Vision-Language Models</li>
            <li>Tiny VLMs: Efficient Multimodal AI at the Edge</li>
            <li>New Benchmark Dataset & Evaluation Metrics</li>
            <li>AI for Sign Language Understanding</li>
            <li>Document Image Processing</li>
            <li>Medical Data Analysis</li>
            <li>Scene Text Detection And Recognition</li>
          </ul>
        </div>

        <p>
          The primary goal of WVLL is to foster a rich exchange of ideas that can crystallize common problems and illuminate promising scientific paradigms in vision-language research. We aim to explicitly contrast competing frameworks, clarify essential research questions, and cultivate a stronger community around these shared interests. WVLL will distinguish itself by its balanced emphasis on theoretical advancements in model design and the practical, societal implications of their deployment, particularly in resource-constrained and specialized domains. We believe this workshop will be highly valuable to the NeurIPS community by providing a focused platform to discuss the frontiers of multimodal AI, encouraging interdisciplinary collaboration, and charting a course towards more comprehensive and responsible vision-language understanding systems. We will encourage the presentation of work-in-progress and forward-looking position papers, fostering a vibrant discussion that looks towards future breakthroughs.
        </p>
      </section>

      <section id="speakers">
        <h2>Invited Speakers</h2>
        <h3>Confirmed Speakers</h3>
        <ul>
          <li><b>Michal Yarom</b>: Research Engineer, Google Research, Israel</li>
          <li><b>Iftekhar Naim</b>: Senior Staff Software Engineer and Manager at Google DeepMind, USA</li>
          <li><b>Junaid Kalia MD</b>: Founder; SaveLife.AI, USA</li>
          <li><b>Veton Kepuska</b>: Professor; Florida Institute of Technology, USA</li>
          <li><b>Lingzi Hong</b>: Assistant Professor; University of North Texas, USA</li>
        </ul>

        <h3>Tentative Speakers</h3>
        <ul>
          <li><b>Mohammad Nurul Huda</b>: Professor, United International University, Bangladesh</li>
          <li><b>Sheak R. Haider Noori</b>: Professor, Daffodil International University, Bangladesh</li>
          <li><b>Angelina Geetha</b>: Professor; Hindustan Institute of Technology and Science, India</li>
          <li><b>Mohammad Lutfi Othman</b>: Professor; Universiti Putra Malaysia, Malaysia</li>
          <li><b>Firoj Alam</b>: Senior Scientist; Qatar Computing Research Institute; Qatar</li>
        </ul>
      </section>

      <section id="diversity-plan">
        <h2>Diversity, Equity & Inclusion Plan</h2>
        <p>
          WVLL 2025 embeds diversity and inclusion across organizers, speakers, and attendees through concrete, realistic actions. Our nine-member committee of three women, one non-binary researcher, and five men spans four continents, balances academia (five members) with industry/NGO roles (four), and blends four seniors with five mid-career scientists, creating natural mentorship pathways and technical breadth from computer vision to clinical AI. We are deliberately recruiting invited speakers through affinity groups and regional mailing lists to secure meaningful representation of women, non-binary scholars, and researchers based in the Global South; early acceptances already span the USA, Malaysia, Portugal, Bangladesh, and China. The gender-neutral CFP explicitly welcomes work on sign-language AI, low-resource languages, and edge deployment in underserved regions, while an optional mentored-review track will pair junior authors with experienced PC members. External sponsorships are being pursued to fund travel stipends prioritized for students from low- and middle-income countries and for caregivers. Live captioning, wheelchair-accessible poster spacing, and an anonymous code-of-conduct reporting channel coordinated by our DEI chair will ensure a safe, inclusive environment, making diversity and broad participation integral to WVLL 2025 rather than an afterthought.
        </p>
      </section>

      <section id="attendees">
        <h2>Estimated Number of Attendees</h2>
        <p>
          Given the growing interest in multimodal AI, particularly in the areas of low-resource language processing, efficient model adaptation, and applied vision-language systems, we anticipate attracting a diverse audience from both academia and industry. Based on the relevance of our topicsâ€”including LLM/VLM architectures, sign language understanding, document image processing, and medical data analysisâ€”we estimate an attendance of approximately 80-100 participants. This includes researchers, practitioners, and students interested in vision-language learning, efficient model design, and AI applications for underrepresented and resource-constrained domains.
        </p>
      </section>

      <section id="requirements">
        <h2>Special Requirements and Technical Needs</h2>
        <p>
          The WVLL workshop will be a one-day, in-person event in accordance with NeurIPS 2025 guidelines. We request a standard A/V setup, including a projector with HDMI input, screen, microphones for both speakers and audience, and stable internet access to support any live demonstrations. We plan to host a poster session and will need space and boards for approximately 8â€“10 physical posters. Additionally, we request a table for showcasing interactive demos related to vision-language systems. While the workshop is fully in-person, we may accommodate up to one hour of remote presentation in the event of unforeseen emergencies, as permitted by NeurIPS. The only additional requirement we foresee is ensuring wheelchair accessibility at the venue.
        </p>
      </section>

      <section id="previous-workshop">
        <h2>Previous Workshop Edition Overview</h2>
        <p>
          This workshop was previously held at WACV 2024, where it focused on vision-language learning for low-resource languages, parameter-efficient model adaptation, and applied multimodal AI. In that edition, we received 14 paper submissions, of which 3 were accepted, resulting in an acceptance rate of approximately 21%. The accepted papers included both extended abstracts and long-format submissions. The authors represented a diverse international background, with submissions from Bangladesh, the United States, and India. The review process was conducted by a panel of 32 expert reviewers from around the world, ensuring a rigorous and fair evaluation process. The workshop was well-received at WACV, and based on the enthusiastic engagement and the growing relevance of our themes, we are now proposing to expand its reach and visibility by bringing it to NeurIPS 2025.
        </p>
        <p><b>URL of previous workshop:</b> <a href="https://wvll.github.io" target="_blank" rel="noopener noreferrer">https://wvll.github.io</a></p>
      </section>

      <section id="organizers">
        <h2>Brief Bios of Organizers</h2>

        <p><b>Fuad Rahman:</b> Fuad Rahman, Ph.D., is an academician and entrepreneur who founded Apurba Technologies, specializing in machine learning. He is also an Adjunct Professor at the University of Arizona's BME Department. His company actively works on computerizing Bangla, a low-resource language, developing the first commercial Bangla OCR and screen reader. He has over 100 peer-reviewed publications.<br>
        Email: <a href="mailto:fuad@apurbatech.com">fuad@apurbatech.com</a> | Website: <a href="http://apurbatech.com" target="_blank" rel="noopener noreferrer">apurbatech.com</a></p>

        <p><b>Syed Akhter Hossain:</b> Dr. Syed Akhter Hossain is the Dean of the Faculty of Science and Information Technologies at Daffodil International University. He has significantly advanced NLP research and has over 250 publications. A recipient of the Best Professor of IT Award (2012) and National ICT Award (2016), he notably developed a machine translator for Bangla Braille.<br>
        Email: <a href="mailto:deanfsit@daffodilvarsity.edu.bd">deanfsit@daffodilvarsity.edu.bd</a> | Website: <a href="https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html" target="_blank" rel="noopener noreferrer">https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html</a></p>

        <p><b>Mouhaydine Tlemcani:</b> Dr. Mouhaydine Tlemcani is an Assistant Professor at the University of Ã‰vora, instrumental in their Mechatronics Engineering program. He holds an M.Sc. (1992) and Ph.D. (2007) in Electrical Engineering. His research includes instrumentation, signal/image processing, embedded systems, and AI applications in engineering, leading projects like non-destructive testing for aeronautic maintenance.<br>
        Email: <a href="mailto:tlem@uevora.pt">tlem@uevora.pt</a> | Website: <a href="https://www.uevora.pt/pessoas?id=5279" target="_blank" rel="noopener noreferrer">https://www.uevora.pt/pessoas?id=5279</a></p>

        <p><b>Tozammel Hossain:</b> Dr. Tozammel Hossain is an Assistant Professor at the University of North Texas, specializing in applied machine learning, causal inference, and biomedical informatics. With a Ph.D. from Virginia Tech and postdoctoral experience at USC, he has contributed to high-impact projects funded by IARPA, DARPA, DHS, and USDA. He has published in leading journals and presented at top conferences.<br>
        Email: <a href="mailto:tozammel.hossain@unt.edu">tozammel.hossain@unt.edu</a> | Website: <a href="https://facultyinfo.unt.edu/faculty-profile?profile=kh0718" target="_blank" rel="noopener noreferrer">https://facultyinfo.unt.edu/faculty-profile?profile=kh0718</a></p>

        <p><b>Tazin Afrin:</b> Dr. Tazin Afrin holds a Ph.D. in Computer Science from the University of Pittsburgh, with expertise in NLP, educational technology, and human-computer interaction. She developed the ArgRewrite revision assistant and published in top-tier venues. At ETS, she develops advanced AI systems using LLMs and machine learning.<br>
        Email: <a href="mailto:tazin.tumpa@gmail.com">tazin.tumpa@gmail.com</a> | Website: <a href="https://tazin-afrin.github.io" target="_blank" rel="noopener noreferrer">https://tazin-afrin.github.io</a></p>

        <p><b>Ting Xiao:</b> Dr. Ting Xiao is an Assistant Professor in Data Science at the University of North Texas (UNT) and Director of the Deep Sensor Information eXtraction (SIX) Lab. She holds a Ph.D. in Physics from Northwestern University. Her research focuses on Machine Learning/Deep Learning, Vector Embeddings, Multimodal Large Language Models, and Clinical/Biomedical AI, with over 100 publications and an h-index of 36.<br>
        Email: <a href="mailto:Ting.Xiao@unt.edu">Ting.Xiao@unt.edu</a> | Website: <a href="https://engineering.unt.edu/people/ting-xiao.html" target="_blank" rel="noopener noreferrer">https://engineering.unt.edu/people/ting-xiao.html</a></p>

        <p><b>Sadia Afroz:</b> Dr. Sadia Afroz is a Lead Scientist at Gen&trade;, leading research in Security and Machine Learning. She holds a Ph.D. in Computer Science from Drexel University, specializing in Computer Security. Her expertise lies at the intersection of security, privacy, and machine learning. She previously served as a Research Professor at ICSI and a Staff Scientist at Avast.<br>
        Email: <a href="mailto:sadia@icsi.berkeley.edu">sadia@icsi.berkeley.edu</a> | Website: <a href="https://www.icsi.berkeley.edu/icsi/people/sadia" target="_blank" rel="noopener noreferrer">https://www.icsi.berkeley.edu/icsi/people/sadia</a></p>

        <p><b>Sheikh Abujar:</b> Sheikh Abujar is a Ph.D. candidate in Computer Science at UAB, researching deep learning, vision-language models (VLMs), and clinical natural language processing. He interned at Samsung Research America (2024) and co-led impactful projects, including creating low-resource datasets like Bayanno (Bangla Speech) and IsharaLipi (Bangla Sign Language).<br>
        Email: <a href="mailto:sabujar@uab.edu">sabujar@uab.edu</a> | Website: <a href="https://sites.google.com/site/iamabujarsheikh" target="_blank" rel="noopener noreferrer">https://sites.google.com/site/iamabujarsheikh</a></p>

        <p><b>AKM Shahariar Azad Rabby:</b> Shahariar Rabby is a researcher at the UAB Lung Imaging Lab and Machine Learning team lead at Apurba Technologies, specializing in OCR, Document Analyses, and Low-Resource Language Vision. He developed "Ekush," the largest Bangla handwritten dataset, and co-founded/supervised the CI LAB and DIU - NLP and Machine Learning Research LAB.<br>
        Email: <a href="mailto:arabby@uab.edu">arabby@uab.edu</a> | Website: <a href="http://rabby.dev" target="_blank" rel="noopener noreferrer">rabby.dev</a></p>

        <p><b>Muntaser Syed:</b> Muntaser Syed is a GPU Developer Advocate at NVIDIA and technical lead for the Open Hackathons team, focusing on accelerating research on supercomputing clusters. A Ph.D. scholar, his interests include machine learning on edge devices, NLP, and speech recognition. He contributed to UAV control systems and the FAA's LAANC program.<br>
        Email: <a href="mailto:muntasers@nvidia.com">muntasers@nvidia.com</a> | Website: <a href="https://www.linkedin.com/in/muntasersyed" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/in/muntasersyed</a></p>
      </section>

      <section id="program-committee">
        <h2>Confirmed Program Committee Members</h2>
        <table class="committee-table">
          <thead>
            <tr>
              <th>Reviewer</th>
              <th>Organization</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>Abdus Sattar</td><td>Daffodil International University, Bangladesh</td></tr>
            <tr><td>Abu Kaisar Mohammad Masum</td><td>Florida Institute of Technology, USA</td></tr>
            <tr><td>Jagdish Chand Bansal</td><td>South Asian University, India</td></tr>
            <tr><td>Stephen Olatunde Olabiyisi</td><td>Ladoke Akintola University of Technology, Nigeria</td></tr>
            <tr><td>Sunil Kumar Khatri</td><td>Amity University Tashkent, Uzbekistan</td></tr>
            <tr><td>Yagyanath Rimal</td><td>Pokhara University, Nepal</td></tr>
            <tr><td>Ghalib Hussaiyn</td><td>PayPal</td></tr>
            <tr><td>Hasmot Ali</td><td>Apurba Technologies Ltd</td></tr>
            <tr><td>Md. Fahad Hossain</td><td>Daffodil International University, Bangladesh</td></tr>
            <tr><td>Mahmudul Hasan</td><td>Comilla University, Bangladesh</td></tr>
            <tr><td>Mohammad Mamun Or Rashid</td><td>Jahangirnagar University, Bangladesh</td></tr>
            <tr><td>Md Majedul Islam</td><td>Kennesaw State University, USA</td></tr>
            <tr><td>Md. Sanzidul Islam</td><td>King Abdulaziz University, Saudi Arabia</td></tr>
            <tr><td>Mirza Sami</td><td>Deka Research & Development</td></tr>
            <tr><td>Mohammad Shorif Uddin</td><td>Jahangirnagar University, Bangladesh</td></tr>
            <tr><td>Mouhaydine Tlemcani</td><td>Universidade de Ã‰vora, Portugal</td></tr>
            <tr><td>Nabeel Mohammed</td><td>North South University, Bangladesh</td></tr>
            <tr><td>Naveed Mahmud</td><td>Florida Institute of Technology, USA</td></tr>
            <tr><td>Nushrat Jahan Ria</td><td>Daffodil International University, Bangladesh</td></tr>
            <tr><td>Pratim Saha</td><td>University of Alabama at Birmingham, USA</td></tr>
            <tr><td>S.R. Subramanya</td><td>National University (San Diego, USA) / Exskillence</td></tr>
            <tr><td>S.M. Saiful Islam Badhon</td><td>University of North Texas, USA</td></tr>
            <tr><td>Saif Islam</td><td>Charles Schwab</td></tr>
            <tr><td>Sandeep Bodduluri</td><td>University of Alabama at Birmingham, USA</td></tr>
            <tr><td>Sharun Akter Khushbu</td><td>Daffodil International University, Bangladesh</td></tr>
            <tr><td>Syed Ashiqur Rahman</td><td>GSK, USA</td></tr>
            <tr><td>Tanvir Ahmed</td><td>University of Central Florida, USA</td></tr>
            <tr><td>S.M. Mazharul Hoque Chowdhury</td><td>University of North Texas, USA</td></tr>
            <tr><td>Monjurul Huda</td><td>Amazon</td></tr>
          </tbody>
        </table>
      </section>
    </main>

    <footer>
        <p style="text-align: center; padding: 20px 0; background-color: var(--nav-bg-color); color: var(--nav-link-color); margin-top: 30px; font-size: 0.9rem;">
            &copy; 2024-2025 WVLL Workshop Organizers. All rights reserved.
        </p>
    </footer>

    <script>
      // Smooth scroll for navigation links
      document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          const hrefAttribute = this.getAttribute('href');
          if (hrefAttribute && hrefAttribute.startsWith('#') && hrefAttribute.length > 1) {
            const targetElement = document.querySelector(hrefAttribute);
            if (targetElement) {
              e.preventDefault();
              targetElement.scrollIntoView({
                behavior: 'smooth'
              });
              // If it's a hamburger menu link, close the menu
              const hamMenuCheckbox = document.getElementById('ham-menu');
              if (hamMenuCheckbox && hamMenuCheckbox.checked) {
                hamMenuCheckbox.checked = false;
              }
            }
          }
        });
      });

      // Update current navigation link based on scroll position
      const sections = document.querySelectorAll('main section[id]');
      const navLinksDesktop = document.querySelectorAll('.desktop-navigation a');
      const navLinksMobile = document.querySelectorAll('.ham-menu a');

      function changeLinkState() {
        let index = sections.length;

        while(--index && window.scrollY + 100 < sections[index].offsetTop) {} // 100px offset
        
        function updateLinks(navLinks) {
          navLinks.forEach((link) => link.classList.remove('current'));
          // Check if the link corresponding to the current section exists
          const currentLink = Array.from(navLinks).find(link => link.getAttribute('href') === '#' + sections[index].id);
          if (currentLink) {
            currentLink.classList.add('current');
          } else if (navLinks.length > 0) { // Fallback to the first link if no section matches (e.g. top of page)
             if (window.scrollY < sections[0].offsetTop - 100) { // If above the first section
                const homeLink = Array.from(navLinks).find(link => link.getAttribute('href') === '#overview');
                if (homeLink) homeLink.classList.add('current');
             }
          }
        }
        
        updateLinks(navLinksDesktop);
        updateLinks(navLinksMobile);
      }

      // Initial call
      if (sections.length > 0) {
        changeLinkState();
        window.addEventListener('scroll', changeLinkState);
      }

    </script>

  </body>
</html>