<!DOCTYPE html>
<html lang="en">
  <head>
    <base href="." />
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css" />
    <meta name="viewport" content="width=device-width" />
    <meta name="resource-type" content="document" />
    <meta name="distribution" content="global" />
    
    <meta name="description" content="Workshop on Vision-Based Understanding and Linguistics (WVLL) - Join us for discussions on vision-based understanding challenges in low-resource languages. Explore document image processing, OCR, NLP, and more." />
    <meta name="keywords" content="Document Image Processing, Physical And Logical Layout Analysis, Optical Character Recognition (OCR), Document Classification, Natural Language Processing (NLP) For Document Understanding, Scene Text Detection And Recognition, Recognition Of Tables And Formulas In Documents, Document Summarization And Translation, Medical Document Analysis, Gold-Standard Benchmarks And Datasets For Low-Resource Languages, Video And Speech Analysis For Low-Resource Languages, Generative AI For Low-Resource Languages" />
    <meta name="author" content="Fuad Rahman, Syed Akhter Hossain, Sheikh Abujar, AKM Shahariar Azad Rabby, Muntaser Syed, Mouhaydine Tlemcani, Tozammel Hossain, Tazin Afrin, Ting Xiao, Sadia Afroz" />
    <meta name="robots" content="index, follow" />
    <meta name="geo.region" content="US-HI" />
    <meta name="geo.placename" content="WAIKOLOA" />
    <meta name="geo.position" content="19.937248;-155.791068" />
    <meta name="ICBM" content="19.937248, -155.791068" />
    <meta property="og:title" content="Workshop on Vision-Based Understanding and Linguistics (WVLL)" />
    <meta property="og:description" content="Join us for discussions on vision-based understanding challenges in low-resource languages. Explore document image processing, OCR, NLP, and more." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://wvll.github.io" />
    <meta property="og:image" content="https://wvll.github.io/assets/conference-image.jpg" />
    <meta property="og:locale" content="en_US" />
    <meta property="article:author" content="https://www.linkedin.com/in/fuadrahman, https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html, https://sites.google.com/site/iamabujarsheikh, https://rabby.dev, https://www.linkedin.com/in/muntasersyed, https://www.uevora.pt/pessoas?id=5279, https://facultyinfo.unt.edu/faculty-profile?profile=kh0718, https://tazin-afrin.github.io, https://engineering.unt.edu/people/ting-xiao.html, https://www.icsi.berkeley.edu/icsi/people/sadia" />
    <meta property="article:publisher" content="https://www.linkedin.com/in/fuadrahman, https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html, https://sites.google.com/site/iamabujarsheikh, https://rabby.dev, https://www.linkedin.com/in/muntasersyed, https://www.uevora.pt/pessoas?id=5279, https://facultyinfo.unt.edu/faculty-profile?profile=kh0718, https://tazin-afrin.github.io, https://engineering.unt.edu/people/ting-xiao.html, https://www.icsi.berkeley.edu/icsi/people/sadia" />
    <meta property="article:published_time" content="2024-01-03" />
    <meta property="article:modified_time" content="2024-01-07" />
    <meta property="og:site_name" content="WVLL Conference" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Workshop on Vision-Based Understanding for Low-Resource Languages (WVLL)" />
    <meta name="twitter:description" content="Join us for discussions on vision-based understanding challenges in low-resource languages. Explore document image processing, OCR, NLP, and more." />
    <meta name="twitter:image" content="https://wvll.github.io/assets/conference-image.jpg" />
    
    <title>WVLL</title>
    <style>
      body {
        font-family: "Times New Roman", serif;
      }

      @media only screen and (max-width: 768px) {
        /* For mobile phones: */
        .container {
          padding: 20px;
        }

        .navigation{
        display: none;
      }
      }

      /* Default style for navigation links */
      .navigation a {
        color: black; /* Default color */
        text-decoration: none; /* Remove underline */
      }

      /* Style for the selected (current) link */
      .navigation a.current {
        color: rgb(24, 134, 134); /* Selected color */
        font-weight: bold; /* Optionally, make the selected link bold */
        /* text-shadow: 2px 2px black */
        /* text-shadow: -1px -1px 0 black, 1px -1px 0 black, -1px 1px 0 black, 1px 1px 0 black; Black outline */
      }
      .title2 {
        color: rgb(24, 134, 134);
      }
      a:hover {
        color: navy;
      }

      .banner {
        position: relative;
        /* border: 2px solid #000;  */
        padding: 8px;
        /* top:2px; */
      }

      

      /* Add CSS for the "top-left" class within the banner */
      .banner .top-left {
        position: absolute; /* Set top-left to absolute positioning */
        top: 8px; /* Adjust top position as needed */
        left: 8px; /* Adjust left position as needed */
        right: 10px; /* Adjust left position as needed */
        /* background-color: #f2f2f2; */
        padding: 10px; /* Add padding to the top-left content */
        width: calc(98.5% - 20px); /* Adjust width as needed */
        height: calc(18% - 20px); /* Adjust height as needed */
        /* border: 2px solid #000;  */
        /* padding: 10px; Add some padding to separate the content from the border */
      }

      .wacv {
        padding: 5px;
        /* border: 2px solid #000;  */
        /* left: 30px */
      }

      .cvf {
        padding: 5px;
        /* border: 2px solid #000;  */
        /* left: 200px */
      }
      .csl {
        /* border: 2px solid #000;  */
        padding: 5px;
        /* height: 50px; */
        /* width: 50px; */
        /* left: 290px */
      }

      .logos {
        position: absolute;
        padding: 8px;
        left: 20px;
        top: 126px;
        background-color: rgba(240, 240, 240, 0.40);
        /* background-color: aliceblue; */
        height: 50px;
        width: 350px;
      }

      .spn {
        position: absolute;
        padding: 8px;
        left: 20px;
        top: 495px;
        /* background-color: aliceblue; */
        height: 60px;
        width: 150px;
      }
      .spn_img{
        position: absolute;
         /* left: 0px; */
        top: 50px; 
        width: 110px;
        height: 20px;
        /* border: 1px solid #000; */
      }
      .spn_txt{
        width: 110px;
        height: 30px;
        /* border: 1px solid #000; */
      }


      :root {
        --home-bg-color: #f2f3ee;
        --menu-bg-color: #cbcbc2;
        --silde-btn-border: #808080;
        --slide-btn-bg: #ddf2db;
        --slide-btn-hoverbg: #f1fff1;
        --alpha-green: rgba(33, 96, 47, 0.51);
        --icon-hover-color: #344a39;
        --icon-hover-bg: #709680;
        --text-color: #616161;
        --border-color: #709680;
        --heading-color: #344a39;
        --box-shadow-color: #b5b5ac;
        --lightest-green: #ecf3ed;
        --light-green: #9ab09a;
        --dark-green: rgba(52, 74, 57, 0.86);
        --box-shadow: 0px 0px 3px 5px var(--box-shadow-color);
        --border-radius: 60px 5px;
        --fade-green: rgba(57, 87, 64, 0.55);
      }

 
      .navigation table{
        display: block;
      }

      /* *,
*::before,
*::after {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
  list-style: none;
} */
      /* body,
html {
  width: 100%;
  font-size: 10px;
  color: var(--text-color);
  font-weight: normal;
  font-family: sans-serif;
  background-color: var(--home-bg-color);
} */

      #ham-menu {
        display: none;
      }
      label[for="ham-menu"] {
        display: block;
        position: relative;
        top: -578px;
        left: 20px;
        z-index: 999;
        width: 60px;
        height: 60px;
        background-color: var(--home-bg-color);
        border-radius: 15px;
        border: 2px solid var(--border-color);
      }
      .ham-menu {
        width: 170vw;
        height: 100%;
        position: fixed;
        top: 0;
        visibility: hidden;
        transform: translate(-110%);
        z-index: 998;
        background-color: var(--lightest-green);
        transition: 1s;
        display: flex;
        /* justify-content: center; */
        /* align-items: center; */
        
      }
      .ham-menu > ul {
        display: flex;
        flex-flow: column nowrap;
        justify-content: space-around;
        padding: 20px;
        height: 50%;
        padding-top: 100px;
      }

      .ham-menu a {
        color: black;
        text-decoration: none;
      }

      .ham-menu a.current {
        color: rgb(24, 134, 134); /* Selected color */
        font-weight: bold; /* Optionally, make the selected link bold */
        /* text-shadow: 2px 2px black */
        /* text-shadow: -1px -1px 0 black, 1px -1px 0 black, -1px 1px 0 black, 1px 1px 0 black; Black outline */
      }

      .ham-menu > ul > li {
        font-size: 5rem;
        white-space: nowrap;
        /* letter-spacing: 0.15em; */
        cursor: pointer;
        color: rgb(97, 97, 97);
        list-style-type: none;
      }
      #ham-menu:checked + label {
        background-color: transparent;
        border-color: var(--dark-green);
      }
      #ham-menu:checked ~ div.ham-menu {
        transform: translate(0px);
        visibility: visible;
      }
      .full-page-green {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        /* background-color: var(--dark-green); */
        z-index: 997;
        opacity: 0;
        visibility: hidden;
        display: none;
        transition: 500ms;
        position: fixed;
        top: 0;
        left: 0;
      }
      #ham-menu:checked ~ div.full-page-green {
        display: block;
        opacity: 1;
        visibility: visible;
      }
      [for="ham-menu"] > div {
        width: 100%;
        height: 100%;
        display: flex;
        flex-flow: column wrap;
        align-content: center;
        align-items: center;
      }
      .menu-line {
        display: block;
        width: 17px;
        height: 2px;
        margin: 10px 0 5px;
        border-top-left-radius: 2px;
        border-bottom-left-radius: 2px;
        background-color: var(--border-color);
        transition: 500ms;
        transform-origin: right center;
      }
      [for="ham-menu"] > div > span:nth-child(4),
      [for="ham-menu"] > div > span:nth-child(5),
      [for="ham-menu"] > div > span:nth-child(6) {
        border-top-left-radius: 0;
        border-bottom-left-radius: 0;
        border-top-right-radius: 2px;
        border-bottom-right-radius: 2px;
        transform-origin: left center;
      }
      #ham-menu:checked + label span {
        background-color: var(--dark-green);
      }
      #ham-menu:checked + label span:nth-child(2),
      #ham-menu:checked + label span:nth-child(5) {
        transform: scale(0);
      }
      #ham-menu:checked + label span:nth-child(1) {
        transform: translateY(17px) rotate(45deg);
      }
      #ham-menu:checked + label span:nth-child(4) {
        transform: translateY(17px) rotate(-45deg);
      }
      #ham-menu:checked + label span:nth-child(3) {
        transform: translateY(-17px) rotate(-45deg);
      }
      #ham-menu:checked + label span:nth-child(6) {
        transform: translateY(-17px) rotate(45deg);
      }

      .hamble {
        @media screen and (min-width: 775px) { 
    display: none;
  }
        }
      

    .committee-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 20px;
    }
    .committee-table th, .committee-table td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
    }
    .committee-table th {
        background-color: #f2f2f2;
    }

    </style>
  </head>

  <body>
    <div class="banner">
      <img
        src="assets/banner.jpg"
        alt="Conference Template Banner"
        style="max-width: 100%"
      />

      <div class="logos">
        <img
          class="wacv"
          src="assets\WACV-Logo_2024.png"
          alt="WACV-Logo_2024"
          style="width: 150px"
        />

        <img
          class="cvf"
          src="assets\CVF-logo.png"
          alt="cvf-Logo_2024"
          style="width: 80px"
        />

        <img
          class="csl"
          src="assets\Computer-Society-Logo.png"
          alt="cs-Logo_2024"
          style="width: 80px"
        />

        
      </div>

      <div class="top-left">
        <span class="title2" style="line-height: 1.33"
          >Workshop on Vision-Based Understanding <br />for Low-Resource
          Languages (WVLL)</span
        >
      </div>
      <div class="bottom-right">January 8, 2024<br />Waikoloa, Hawaii</div>
    </div>

    <div class="hamble">
      <input type="checkbox" id="ham-menu" />
     
        <label for="ham-menu">
          <div class="hide-des">
            <span class="menu-line"></span>
            <span class="menu-line"></span>
            <span class="menu-line"></span>
            <span class="menu-line"></span>
            <span class="menu-line"></span>
            <span class="menu-line"></span>
          </div>
        </label>
      
        
        <div class="full-page-green"></div>
   
        
        <div class="ham-menu">
          <ul class="centre-text bold-text">
            <li><a class="current" title="Conference Home Page" href="#overview">Home</a></li>
            <li><a title="About the Conference" href="#overview">About</a></li>
            <li>
              <a title="Organizers of the Conference" href="#organizers"
                >Organizers</a
              >
            </li>
            <li><a title="Conference Program" href="#speakers">Program</a></li>
            <li>
              <a  title="Conference Sponsors" href="#previous-workshop"
                >Sponsors</a
              >
            </li>
            <li>
              <a title="Conference Social Impact" href="#diversity-plan"
                >Social Impact</a
              >
            </li>
             <li>
              <a title="Program Committee" href="#program-committee"
                >Program Committee</a
              >
            </li>
          </ul>
        </div>
      </div>

    <div class="container">
      <h2 style="color: rgb(24, 134, 134)" id="overview">Workshop Overview</h2>
      <p style="text-align: justify">
        The ``2\textsuperscript{nd} Workshop on Vision-Based Understanding and Linguistics (WVLL)'' aims to create a dynamic and interactive forum for researchers exploring the rapidly evolving intersection of computer vision, natural language processing, and linguistic principles to achieve deeper and more nuanced machine understanding. As vision-language models (VLMs) demonstrate increasingly sophisticated capabilities, WVLL will focus on the critical challenges and opportunities that lie ahead, emphasizing the development of models that are not only powerful but also efficient, equitable, and grounded in a robust understanding of both visual and linguistic structures.
      </p>

      <h2 style="color: rgb(24, 134, 134)">Key Areas of Exploration:</h2>
      <div style="column-count: 2;">
        <ul>
          <li>AI For Low-Resource Languages</li>
          <li>Video And Speech Analysis For Low-Resource Languages</li>
          <li>LLM and VLM Architectures and Neural Design</li>
          <li>Parameter-Efficient Adaptation of Large Vision-Language Models</li>
          <li>Applications in Vision-Language Models</li>
          <li>Tiny VLMs: Efficient Multimodal AI at the Edge</li>
          <li>New Benchmark Dataset \& Evaluation Metrics</li>
          <li>AI for Sign Language Understanding</li>
          <li>Document Image Processing</li>
          <li>Medical Data Analysis</li>
          <li>Scene Text Detection And Recognition</li>
        </ul>
      </div>

      <p style="text-align: justify">
        The primary goal of WVLL is to foster a rich exchange of ideas that can crystallize common problems and illuminate promising scientific paradigms in vision-language research. We aim to explicitly contrast competing frameworks, clarify essential research questions, and cultivate a stronger community around these shared interests. WVLL will distinguish itself by its balanced emphasis on theoretical advancements in model design and the practical, societal implications of their deployment, particularly in resource-constrained and specialized domains. We believe this workshop will be highly valuable to the NeurIPS community by providing a focused platform to discuss the frontiers of multimodal AI, encouraging interdisciplinary collaboration, and charting a course towards more comprehensive and responsible vision-language understanding systems. We will encourage the presentation of work-in-progress and forward-looking position papers, fostering a vibrant discussion that looks towards future breakthroughs.
      </p>

      <h2 style="color: rgb(24, 134, 134)" id="speakers">Invited Speakers</h2>
      <h3>Confirmed Speakers</h3>
      <ul>
        <li><b>Michal Yarom</b>: Research Engineer, Google Research, Israel</li>
        <li><b>Iftekhar Naim</b>: Senior Staff Software Engineer and Manager at Google DeepMind, USA</li>
        <li><b>Junaid Kalia MD</b>: Founder; SaveLife.AI, USA</li>
        <li><b>Veton Kepuska</b>: Professor; Florida Institute of Technology, USA</li>
        <li><b>Lingzi Hong</b>: Assistant Professor; University of North Texas, USA</li>
      </ul>

      <h3>Tentative Speakers</h3>
      <ul>
        <li><b>Mohammad Nurul Huda</b>: Professor, United International University, Bangladesh</li>
        <li><b>Sheak R. Haider Noori</b>: Professor, Daffodil International University, Bangladesh</li>
        <li><b>Angelina Geetha</b>: Professor; Hindustan Institute of Technology and Science, India</li>
        <li><b>Mohammad Lutfi Othman</b>: Professor; Universiti Putra Malaysia, Malaysia</li>
        <li><b>Firoj Alam</b>: Senior Scientist; Qatar Computing Research Institute; Qatar</li>
      </ul>

      <h2 style="color: rgb(24, 134, 134)" id="diversity-plan">Diversity, Equity \& Inclusion Plan</h2>
      <p style="text-align: justify">
        WVLL 2025 embeds diversity and inclusion across organizers, speakers, and attendees through concrete, realistic actions. Our nine-member committee of three women, one non-binary researcher, and five men spans four continents, balances academia (five members) with industry/NGO roles (four), and blends four seniors with five mid-career scientists, creating natural mentorship pathways and technical breadth from computer vision to clinical AI. We are deliberately recruiting invited speakers through affinity groups and regional mailing lists to secure meaningful representation of women, non-binary scholars, and researchers based in the Global South; early acceptances already span the USA, Malaysia, Portugal, Bangladesh, and China. The gender-neutral CFP explicitly welcomes work on sign-language AI, low-resource languages, and edge deployment in underserved regions, while an optional mentored-review track will pair junior authors with experienced PC members. External sponsorships are being pursued to fund travel stipends prioritized for students from low- and middle-income countries and for caregivers. Live captioning, wheelchair-accessible poster spacing, and an anonymous code-of-conduct reporting channel coordinated by our DEI chair will ensure a safe, inclusive environment, making diversity and broad participation integral to WVLL 2025 rather than an afterthought.
      </p>

      <h2 style="color: rgb(24, 134, 134)">Estimated Number of Attendees</h2>
      <p style="text-align: justify">
        Given the growing interest in multimodal AI, particularly in the areas of low-resource language processing, efficient model adaptation, and applied vision-language systems, we anticipate attracting a diverse audience from both academia and industry. Based on the relevance of our topics—including LLM/VLM architectures, sign language understanding, document image processing, and medical data analysis—we estimate an attendance of approximately 80-100 participants. This includes researchers, practitioners, and students interested in vision-language learning, efficient model design, and AI applications for underrepresented and resource-constrained domains.
      </p>

      <h2 style="color: rgb(24, 134, 134)">Special Requirements and Technical Needs</h2>
      <p style="text-align: justify">
        The WVLL workshop will be a one-day, in-person event in accordance with NeurIPS 2025 guidelines. We request a standard A/V setup, including a projector with HDMI input, screen, microphones for both speakers and audience, and stable internet access to support any live demonstrations. We plan to host a poster session and will need space and boards for approximately 8–10 physical posters. Additionally, we request a table for showcasing interactive demos related to vision-language systems. While the workshop is fully in-person, we may accommodate up to one hour of remote presentation in the event of unforeseen emergencies, as permitted by NeurIPS. The only additional requirement we foresee is ensuring wheelchair accessibility at the venue.
      </p>

      <h2 style="color: rgb(24, 134, 134)" id="previous-workshop">Previous Workshop Edition Overview</h2>
      <p style="text-align: justify">
        This workshop was previously held at WACV 2024, where it focused on vision-language learning for low-resource languages, parameter-efficient model adaptation, and applied multimodal AI. In that edition, we received 14 paper submissions, of which 3 were accepted, resulting in an acceptance rate of approximately 21\%. The accepted papers included both extended abstracts and long-format submissions. The authors represented a diverse international background, with submissions from Bangladesh, the United States, and India. The review process was conducted by a panel of 32 expert reviewers from around the world, ensuring a rigorous and fair evaluation process. The workshop was well-received at WACV, and based on the enthusiastic engagement and the growing relevance of our themes, we are now proposing to expand its reach and visibility by bringing it to NeurIPS 2025.
      </p>
      <p><b>URL of previous workshop:</b> <a href="https://wvll.github.io">https://wvll.github.io</a></p>

      <h2 style="color: rgb(24, 134, 134)" id="organizers">Brief Bios of Organizers</h2>

      <p><b>Fuad Rahman:</b> Fuad Rahman, Ph.D., is an academician and entrepreneur who founded Apurba Technologies, specializing in machine learning. He is also an Adjunct Professor at the University of Arizona's BME Department. His company actively works on computerizing Bangla, a low-resource language, developing the first commercial Bangla OCR and screen reader. He has over 100 peer-reviewed publications.<br>
      Email: <a href="mailto:fuad@apurbatech.com">fuad@apurbatech.com</a> | Website: <a href="http://apurbatech.com">apurbatech.com</a></p>

      <p><b>Syed Akhter Hossain:</b> Dr. Syed Akhter Hossain is the Dean of the Faculty of Science and Information Technologies at Daffodil International University. He has significantly advanced NLP research and has over 250 publications. A recipient of the Best Professor of IT Award (2012) and National ICT Award (2016), he notably developed a machine translator for Bangla Braille.<br>
      Email: <a href="mailto:deanfsit@daffodilvarsity.edu.bd">deanfsit@daffodilvarsity.edu.bd</a> | Website: <a href="https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html">https://faculty.daffodilvarsity.edu.bd/profile/swe/akhter.html</a></p>

      <p><b>Mouhaydine Tlemcani:</b> Dr. Mouhaydine Tlemcani is an Assistant Professor at the University of Évora, instrumental in their Mechatronics Engineering program. He holds an M.Sc. (1992) and Ph.D. (2007) in Electrical Engineering. His research includes instrumentation, signal/image processing, embedded systems, and AI applications in engineering, leading projects like non-destructive testing for aeronautic maintenance.<br>
      Email: <a href="mailto:tlem@uevora.pt">tlem@uevora.pt</a> | Website: <a href="https://www.uevora.pt/pessoas?id=5279">https://www.uevora.pt/pessoas?id=5279</a></p>

      <p><b>Tozammel Hossain:</b> Dr. Tozammel Hossain is an Assistant Professor at the University of North Texas, specializing in applied machine learning, causal inference, and biomedical informatics. With a Ph.D. from Virginia Tech and postdoctoral experience at USC, he has contributed to high-impact projects funded by IARPA, DARPA, DHS, and USDA. He has published in leading journals and presented at top conferences.<br>
      Email: <a href="mailto:tozammel.hossain@unt.edu">tozammel.hossain@unt.edu</a> | Website: <a href="https://facultyinfo.unt.edu/faculty-profile?profile=kh0718">https://facultyinfo.unt.edu/faculty-profile?profile=kh0718</a></p>

      <p><b>Tazin Afrin:</b> Dr. Tazin Afrin holds a Ph.D. in Computer Science from the University of Pittsburgh, with expertise in NLP, educational technology, and human-computer interaction. She developed the ArgRewrite revision assistant and published in top-tier venues. At ETS, she develops advanced AI systems using LLMs and machine learning.<br>
      Email: <a href="mailto:tazin.tumpa@gmail.com">tazin.tumpa@gmail.com</a> | Website: <a href="https://tazin-afrin.github.io">https://tazin-afrin.github.io</a></p>

      <p><b>Ting Xiao:</b> Dr. Ting Xiao is an Assistant Professor in Data Science at the University of North Texas (UNT) and Director of the Deep Sensor Information eXtraction (SIX) Lab. She holds a Ph.D. in Physics from Northwestern University. Her research focuses on Machine Learning/Deep Learning, Vector Embeddings, Multimodal Large Language Models, and Clinical/Biomedical AI, with over 100 publications and an h-index of 36.<br>
      Email: <a href="mailto:Ting.Xiao@unt.edu">Ting.Xiao@unt.edu</a> | Website: <a href="https://engineering.unt.edu/people/ting-xiao.html">https://engineering.unt.edu/people/ting-xiao.html</a></p>

      <p><b>Sadia Afroz:</b> Dr. Sadia Afroz is a Lead Scientist at Gen&trade;, leading research in Security and Machine Learning. She holds a Ph.D. in Computer Science from Drexel University, specializing in Computer Security. Her expertise lies at the intersection of security, privacy, and machine learning. She previously served as a Research Professor at ICSI and a Staff Scientist at Avast.<br>
      Email: <a href="mailto:sadia@icsi.berkeley.edu">sadia@icsi.berkeley.edu</a> | Website: <a href="https://www.icsi.berkeley.edu/icsi/people/sadia">https://www.icsi.berkeley.edu/icsi/people/sadia</a></p>

      <p><b>Sheikh Abujar:</b> Sheikh Abujar is a Ph.D. candidate in Computer Science at UAB, researching deep learning, vision-language models (VLMs), and clinical natural language processing. He interned at Samsung Research America (2024) and co-led impactful projects, including creating low-resource datasets like Bayanno (Bangla Speech) and IsharaLipi (Bangla Sign Language).<br>
      Email: <a href="mailto:sabujar@uab.edu">sabujar@uab.edu</a> | Website: <a href="https://sites.google.com/site/iamabujarsheikh">https://sites.google.com/site/iamabujarsheikh</a></p>

      <p><b>AKM Shahariar Azad Rabby:</b> Shahariar Rabby is a researcher at the UAB Lung Imaging Lab and Machine Learning team lead at Apurba Technologies, specializing in OCR, Document Analyses, and Low-Resource Language Vision. He developed "Ekush," the largest Bangla handwritten dataset, and co-founded/supervised the CI LAB and DIU - NLP and Machine Learning Research LAB.<br>
      Email: <a href="mailto:arabby@uab.edu">arabby@uab.edu</a> | Website: <a href="http://rabby.dev">rabby.dev</a></p>

      <p><b>Muntaser Syed:</b> Muntaser Syed is a GPU Developer Advocate at NVIDIA and technical lead for the Open Hackathons team, focusing on accelerating research on supercomputing clusters. A Ph.D. scholar, his interests include machine learning on edge devices, NLP, and speech recognition. He contributed to UAV control systems and the FAA's LAANC program.<br>
      Email: <a href="mailto:muntasers@nvidia.com">muntasers@nvidia.com</a> | Website: <a href="https://www.linkedin.com/in/muntasersyed">https://www.linkedin.com/in/muntasersyed</a></p>

      <h2 style="color: rgb(24, 134, 134)" id="program-committee">Confirmed Program Committee Members</h2>
      <table class="committee-table">
        <thead>
          <tr>
            <th>Reviewer</th>
            <th>Organization</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>Abdus Sattar</td><td>Daffodil International University, Bangladesh</td></tr>
          <tr><td>Abu Kaisar Mohammad Masum</td><td>Florida Institute of Technology, USA</td></tr>
          <tr><td>Jagdish Chand Bansal</td><td>South Asian University, India</td></tr>
          <tr><td>Stephen Olatunde Olabiyisi</td><td>Ladoke Akintola University of Technology, Nigeria</td></tr>
          <tr><td>Sunil Kumar Khatri</td><td>Amity University Tashkent, Uzbekistan</td></tr>
          <tr><td>Yagyanath Rimal</td><td>Pokhara University, Nepal</td></tr>
          <tr><td>Ghalib Hussaiyn</td><td>PayPal</td></tr>
          <tr><td>Hasmot Ali</td><td>Apurba Technologies Ltd</td></tr>
          <tr><td>Md. Fahad Hossain</td><td>Daffodil International University, Bangladesh</td></tr>
          <tr><td>Mahmudul Hasan</td><td>Comilla University, Bangladesh</td></tr>
          <tr><td>Mohammad Mamun Or Rashid</td><td>Jahangirnagar University, Bangladesh</td></tr>
          <tr><td>Md Majedul Islam</td><td>Kennesaw State University, USA</td></tr>
          <tr><td>Md. Sanzidul Islam</td><td>King Abdulaziz University, Saudi Arabia</td></tr>
          <tr><td>Mirza Sami</td><td>Deka Research & Development</td></tr>
          <tr><td>Mohammad Shorif Uddin</td><td>Jahangirnagar University, Bangladesh</td></tr>
          <tr><td>Mouhaydine Tlemcani</td><td>Universidade de Évora, Portugal</td></tr>
          <tr><td>Nabeel Mohammed</td><td>North South University, Bangladesh</td></tr>
          <tr><td>Naveed Mahmud</td><td>Florida Institute of Technology, USA</td></tr>
          <tr><td>Nushrat Jahan Ria</td><td>Daffodil International University, Bangladesh</td></tr>
          <tr><td>Pratim Saha</td><td>University of Alabama at Birmingham, USA</td></tr>
          <tr><td>S.R. Subramanya</td><td>National University (San Diego, USA) / Exskillence</td></tr>
          <tr><td>S.M. Saiful Islam Badhon</td><td>University of North Texas, USA</td></tr>
          <tr><td>Saif Islam</td><td>Charles Schwab</td></tr>
          <tr><td>Sandeep Bodduluri</td><td>University of Alabama at Birmingham, USA</td></tr>
          <tr><td>Sharun Akter Khushbu</td><td>Daffodil International University, Bangladesh</td></tr>
          <tr><td>Syed Ashiqur Rahman</td><td>GSK, USA</td></tr>
          <tr><td>Tanvir Ahmed</td><td>University of Central Florida, USA</td></tr>
          <tr><td>S.M. Mazharul Hoque Chowdhury</td><td>University of North Texas, USA</td></tr>
          <tr><td>Monjurul Huda</td><td>Amazon</td></tr>
        </tbody>
      </table>

    </div>
  </body>
</html>